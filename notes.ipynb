{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One bit is the amount of information required to choose between two equally probable alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose from *m* equally probably alternatives you need log*n* bits of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random variable *X* is a variable whose possible values are numerical outcomes of a random phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *source* generates *messages*. A *message* is an ordered sequence of symbols where each symbol corresponds to\n",
    "the value of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message comprising symbols $s = (s_1, ..., s_k)$ is\n",
    "encoded by a function $x = g(s)$ into a sequence of codewords\n",
    "$x = (x_1, ... , x_n)$, where the number of symbols and codewords\n",
    "are not necessarily equal. These codewords are transmitted\n",
    "through a communication channel to produce outputs $y =\n",
    "(y_1, ... , y_n)$ which are decoded to recover the message $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of each symbol being generated by the source is defined by the probability distribution.  \n",
    "$p(S) = p(s_1), ... , p(s_\\alpha)$  \n",
    "where, by definition, the sum of p(s) values must add up to one,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Shannon* is a unit of information. Due to unprobable events conveying more information than probable ones, it is said to be correlated to the measure of suprise or uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon information of a particular outcome is:  \n",
    "$h(x) = -log_2 \\ p(x) \\ bits$  \n",
    "where *h* is standard notation for Shannon information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent to the variable's possible outcomes.\n",
    "\n",
    "$ H(X) \\approx -\\sum_{x\\in X} p(x) \\ log \\ {p(x)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable with an entropy of $H(X)$ bits provides\n",
    "enough Shannon information to choose between $m = 2^{H(X)}$\n",
    "equally probable alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.4689955935892812\n",
      "We could represent the information of 1000 biased coin flips using as little as 469 bits\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def calcualte_entropy(*probabilties):\n",
    "  return -sum(\n",
    "    map( lambda p : p * math.log2(p), probabilties)\n",
    "  )\n",
    "\n",
    "print(calcualte_entropy(0.5, 0.5))\n",
    "print(calcualte_entropy(0.9, 0.1))\n",
    "print(f\"We could represent the information of 1000 biased coin flips using as little as {round(1000 * calcualte_entropy(0.9, 0.1))} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No biased coin can have an average uncertainty greater than that of an unbiased coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source coding theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The capacity $C$ of a noiseless channel is the maximum number of bits it can communicate per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon's source coding theorem:  \n",
    "Let a source have entropy $H$ (bits per symbol) and a channel\n",
    "have a capacity $C$ (bits per second). Then it is possible to\n",
    "encode the output of the source in such a way as to transmit\n",
    "at the average rate $C/H - \\epsilon$ symbols per second over the\n",
    "channel where $\\epsilon$ is arbitrarily small. It is not possible to\n",
    "transmit at an average rate greater than $C/H$ (symbols/s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If each independently chosen value of a variable represents a non-integer ammount of bits then an efficient\n",
    "encoding can be obtained by combining several symbols in a single binary codeword.\n",
    "\n",
    "For example: If we have a 6 sided dice the entropy for this is $H(x) =  2.58$. If we use 3 bits to encode each dice we will not be transfering the data efficiently since some of the bits are wasted. To improve efficiency we can instead send the outcome values of three throws at a time using 8 bits. (This is possible because $6 * 6 * 6 = 216 < 256 = 2^8$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number such as $\\pi$ can be represented with a computer program containing finite ammount of bits. Thus, the\n",
    "infinite number of digits in $\\pi$ can be compressed to $n_\\pi$ bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Channel Coding Theorem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Uncertainty which arises by virtue of freedom of choice on\n",
    "the part of the sender is desirable uncertainty.  \n",
    "Uncertainty which arises because of errors or because of the influence of noise is undesirable.\"  \n",
    "Weaver W, 1949."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information is a general measure of association between two variables. Given two variables $X$ and $Y$ , the mutual information $I(X, Y)$\n",
    "between them is the average information that we gain about $Y$ after we have observed a single value of $X$. Mutual information is a symetric quantity which means it is also the average information that we gain about $X$ after we have observed a single value of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\noisy-channel.PNG\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of residual uncertainty we have about the value of $Y$\n",
    "after observing a single value of $X$ is the conditional entropy $H(Y|X)$. Because we usually consider noise $\\eta$ as being added to the input as it\n",
    "passes through the channel, this particular conditional entropy $H(Y|X)$\n",
    "is also called the noise entropy, $H(\\eta) = H(Y|X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional entropy $H(Y|X)$ is the average\n",
    "uncertainty in $Y$ after $X$ is observed, and is therefore the\n",
    "average uncertainty in $Y$ that cannot be attributed to $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X$ and $Y$ are independent then the entropy of\n",
    "the joint distribution $p(X, Y)$ is equal to the summed entropies\n",
    "of its marginal distributions, $H(X, Y) = H(X) + H(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(X, Y ) = H(X) + H(Y) - I(X, Y)$\n",
    "\n",
    "The mutual information between two variables $X$\n",
    "and $Y$ is the average reduction in uncertainty about the value\n",
    "of $X$ provided by the value of $Y$ , and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a communication channel with input $X$ and\n",
    "output $Y = X + \\eta$, the conditional entropy $H(Y|X)$ is the\n",
    "entropy of the channel noise $H(\\eta)$ added to $X$ by the channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\relationship.PNG\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon's fundamental theorem for a discrete channel with noise: In essence, Shannon's theorem states that it is possible to use a\n",
    "communication channel to communicate information with a low error\n",
    "rate $\\epsilon$, at a rate R arbitrarily close to the channel capacity of\n",
    "$C$ bits/s, but it is not possible to communicate information at a rate\n",
    "greater than $C$ bits/s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy Typewriter:the noisy typewriter produces letters (outputs) that are\n",
    "unreliably related to the (input) letter typed. Specifically, each typed\n",
    "letter produces one of three letters that are near (alphabetically) to\n",
    "the typed letter. However, we can make this particular noisy communication channel\n",
    "communicate information without any error (i.e. with $\\epsilon$ = 0). If we\n",
    "restrict the inputs to every third letter in the alphabet. In this way, we have effectively transformed a noisy typewriter\n",
    "into a fully functional error-free communication channel.\n",
    "\n",
    "\n",
    "In this example, the error rate is zero. However, what makes the\n",
    "noisy coding theorem remarkable is that Shannon proved the error rate\n",
    "can be reduced to an arbitrarily small value for any data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
